<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>
  </head>
  <body>
    <h1>
      <strong
        >Assignment 2 – Hybrid RAG System with Automated Evaluation</strong
      >
    </h1>
    <h2><strong>Important Dates</strong></h2>
    <ul>
      <li>Roll-out Date: <strong>15 January 2026</strong></li>
      <li>Doubt Clarification Deadline: <strong>22 January 2026</strong></li>
      <li>Submission Deadline: <strong>8 February 2026</strong></li>
    </ul>
    <p>
      <em
        ><strong
          >Please Note: All clarifications must be raised strictly within the
          first 7 days from the assignment roll-out, i.e., by 22 January 2026.
          Any questions submitted after this date will not be
          entertained.</strong
        ></em
      >
    </p>
    <h2><strong>Objective</strong></h2>
    <p>
      Build a Hybrid Retrieval-Augmented Generation (RAG) system combining dense
      vector retrieval, sparse keyword retrieval (BM25), and Reciprocal Rank
      Fusion (RRF) to answer questions from 500 Wikipedia articles. Evaluate
      using an automated framework with 100 generated questions.
    </p>
    <h2><strong>Dataset Requirements</strong></h2>
    <p>Wikipedia URL Collection (500 URLs per group):</p>
    <p>
      Fixed Set (200 URLs): Each group must first sample a unique set of 200
      Wikipedia URLs (minimum 200 words per page) covering diverse topics. Store
      these in a JSON file (fixed_urls.json).
      <strong>No two groups should share the same 200 URLs.</strong> These URLs
      remain constant across all indexing operations.
    </p>
    <p>
      Random Set (300 URLs): For each indexing run, randomly sample 300
      additional Wikipedia URLs (minimum 200 words per page). These should
      change every time the system is rebuilt/indexed.
    </p>
    <p>
      Total Corpus: 200 fixed + 300 random = 500 URLs. Extract, clean, and chunk
      the text (200-400 tokens with 50-token overlap). Store with metadata (URL,
      title, unique chunk IDs).
    </p>
    <h2><strong>Part 1: Hybrid RAG System (10 Marks)</strong></h2>
    <h3><strong>1.1 Dense Vector Retrieval</strong></h3>
    <p>
      Use a sentence embedding model (e.g., all-MiniLM-L6-v2, all-mpnet-base-v2)
      to embed chunks. Build a vector index (FAISS/ChromaDB) and retrieve top-K
      chunks via cosine similarity.
    </p>
    <h3><strong>1.2 Sparse Keyword Retrieval</strong></h3>
    <p>
      Implement BM25 algorithm for keyword-based retrieval. Build index over
      chunks and retrieve top-K results.
    </p>
    <h3><strong>1.3 Reciprocal Rank Fusion (RRF)</strong></h3>
    <p>
      For each query, retrieve top-K chunks from both dense and sparse methods.
      Combine using RRF: RRF_score(d) = Σ 1/(k + rank_i(d)) where k=60. Select
      top-N chunks by RRF score for final context.
    </p>
    <h3><strong>1.4 Response Generation</strong></h3>
    <p>
      Use an open-source LLM (e.g., DistilGPT2, Flan-T5-base, Llama-2-7B).
      Concatenate top-N chunks with query and generate answers within context
      limits.
    </p>
    <h3><strong>1.5 User Interface</strong></h3>
    <p>
      Build with Streamlit/Gradio/Flask. Display: user query input, generated
      answer, top retrieved chunks with sources, dense/sparse/RRF scores, and
      response time.
    </p>
    <h2><strong>Part 2: Automated Evaluation (6 + 4 Marks)</strong></h2>
    <h3><strong>2.1 Question Generation (Automated)</strong></h3>
    <p>
      Generate 100 Q&amp;A pairs from Wikipedia corpus using LLMs or extraction
      methods. Include diverse question types: factual, comparative,
      inferential, multi-hop. Store with ground truth, source IDs, and question
      categories.
    </p>
    <h3><strong>2.2 Evaluation Metrics</strong></h3>
    <h3><strong>2.2.1 Mandatory Metric (Basic - 2 Marks)</strong></h3>
    <p>
      Mean Reciprocal Rank (MRR) - URL Level: Calculate MRR at the URL level
      (not chunk level). For each question, find the rank position of the first
      correct Wikipedia URL in the retrieved results. MRR = average of 1/rank
      across all questions. This measures how quickly the system identifies the
      correct source document.
    </p>
    <h3><strong>2.2.2 Additional Custom Metrics (4 Marks)</strong></h3>
    <p>
      Select and implement 2 additional metrics beyond the mandatory MRR. For
      each metric, you must:
    </p>
    <ol>
      <li>
        Justify Selection: Explain why this metric is important for evaluating
        your RAG system.
      </li>
      <li>
        Calculation Method: Provide detailed mathematical formulation and
        implementation details.
      </li>
      <li>
        Interpretation: Explain what the scores indicate about system
        performance and how to interpret results.
      </li>
    </ol>
    <p>
      Suggested metric categories: Answer quality (EM, F1, BLEU, ROUGE,
      BERTScore, Semantic Similarity), Retrieval quality (Precision@K, Recall@K,
      NDCG@K, Hit Rate), Context relevance (Contextual Precision, Contextual
      Recall), Faithfulness (hallucination detection, answer grounding),
      Efficiency (response time, latency breakdown), or Novel custom metrics
      designed by your team.
    </p>
    <h3><strong>2.3 Innovative Evaluation (4 Marks)</strong></h3>
    <p>Demonstrate creativity through advanced techniques such as:</p>
    <ul>
      <li>
        Adversarial Testing: Challenging questions (ambiguous, negated,
        multi-hop), paraphrasing robustness, unanswerable questions for
        hallucination detection.
      </li>
      <li>
        Ablation Studies: Compare dense-only, sparse-only, and hybrid
        performance. Experiment with different K, N, and RRF k values.
      </li>
      <li>
        Error Analysis: Categorize failures (retrieval, generation, context
        issues) by question type with visualizations.
      </li>
      <li>
        LLM-as-Judge: Use LLM to evaluate factual accuracy, completeness,
        relevance, and coherence with automated explanations.
      </li>
      <li>
        Confidence Calibration: Estimate answer confidence and measure
        correlation with correctness using calibration curves.
      </li>
      <li>
        Novel Metrics: Custom metrics for entity coverage, answer diversity,
        hallucination rate, or temporal consistency.
      </li>
      <li>
        Interactive Dashboard: Real-time metrics, question breakdowns, retrieval
        visualizations, and method comparisons.
      </li>
    </ul>
    <h3><strong>2.4 Automated Pipeline</strong></h3>
    <p>
      Build a single-command pipeline that loads questions, runs a RAG system,
      computes all metrics, and generates comprehensive reports (PDF/HTML) with
      structured output (CSV/JSON).
    </p>
    <h3><strong>2.5 Evaluation Report Contents</strong></h3>
    <p>
      Overall performance summary with MRR and custom metrics averages. Detailed
      justification for the 2 selected custom metrics including: why chosen,
      calculation methodology, and interpretation guidelines. Results table
      (Question ID, Question, Ground Truth, Generated Answer, MRR, Custom Metric
      1, Custom Metric 2, Time). Visualizations: metric comparisons, score
      distributions, retrieval heatmaps, response times, ablation results. Error
      analysis with failure examples and patterns.
    </p>
    <h2><strong>Submission Requirements</strong></h2>
    <p>Submit one ZIP file per group: Group_&lt;Number&gt;_Hybrid_RAG.zip</p>
    <p>Required Contents:</p>
    <ol>
      <li>
        <a
          class="autolink"
          title="Code"
          href="https://taxila-aws.bits-pilani.ac.in/mod/resource/view.php?id=289468"
          >Code</a
        >
        (.ipynb or .py): Complete RAG implementation (data collection,
        preprocessing, dense/sparse retrieval, RRF, generation) with detailed
        comments and markdown.
      </li>
      <li>
        Evaluation: Question generation script, 100-question dataset (JSON/CSV),
        evaluation pipeline, metrics implementation, and innovative components.
      </li>
      <li>
        Report (PDF): Architecture diagram, evaluation results with
        tables/visualizations, innovative approach description, ablation
        studies, error analysis, and 3+ system screenshots.
      </li>
      <li>
        Interface: Hosted app link (Streamlit/Gradio/HF Spaces) OR standalone
        app with setup instructions.
      </li>
      <li>
        README.md: Installation steps, dependencies, run instructions (system +
        evaluation), and fixed 200 Wikipedia URLs list in JSON format.
      </li>
      <li>
        Data: fixed_urls.json (200 URLs), preprocessed corpus, vector database,
        100-question dataset, and evaluation results (all files or regeneration
        instructions).
      </li>
    </ol>
    <h2>Technical Notes</h2>
    <p>
      Required Libraries: sentence-transformers, faiss-cpu, rank-bm25,
      transformers, wikipedia-api, beautifulsoup4, nltk, rouge-score,
      bert-score, scikit-learn.
    </p>
    <p>
      Resources: Google Colab, Kaggle Notebooks, or campus GPU clusters. Use
      only open-source models and tools.
    </p>
    <p>
      Key to Success: Innovation in evaluation is critical for full marks. Think
      beyond standard metrics, be creative!
    </p>
    <p>Good luck!</p>
  </body>
</html>
